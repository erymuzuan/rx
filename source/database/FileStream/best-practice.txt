Best Practices

When you work with FILESTREAM, there are a few tips and best practices that can improve your experience. First, consider the overkill factor. If you are dealing with very small BLOBs, and by that I mean less than a MB, you'll actually be better off using ordinary varbinary(max) columns to store BLOBs in off-row pages within the standard database filegroups. With such modestly-sized BLOB, there's a much lower chance of bloating the structured filegroups and introducing FILESTREAM could wind up being more effort than it's worth. If you're dealing with many BLOBs larger than a MB, then migrating to FILESTREAM will enable you to scale like you couldn't possibly before. At that point, you can enjoy the transparency of T-SQL access, but you also need to be wary of the memory pressure on SQL Server when accessing BLOBs with T-SQL. The general rule of thumb here is that you should avoid T-SQL access and only use the streaming API as you've seen with SqlFileStream. It's hard to tell exactly where the tipping point lies because it's about more than just BLOB size, it's also about how frequent your access is. But at the end of the day, the streaming API is always preferred over T-SQL because it relieves SQL Server of a lot of work and memory when you're handling BLOBs, and shifts that burden over to client applications. Some types of BLOBs are useless unless they are processed in their entirety, but in other cases it can be helpful to retrieve just the first piece and maybe only the first few bytes of the BLOB. One obvious example could be large text files. You can read the first few bytes to offer up a preview of the text file to the user without having to load the entire BLOB. For these scenarios, you can use T-SQL access rather than SqlFileStream, and limit the amount of BLOB data returned by using the SUBSTRING function. This avoids the memory pressure problem because SQL Server will only read as much of the file as it needs to, and not the whole thing. To support 16-bit applications, NTFS creates short file names for each file that gets created. This second file name follows the legacy 8.3 naming convention used by 16-bit applications. FILESTREAM only runs on the 32-bit version of SQL Server and, though it's possible, it's highly unlikely that other 16-bit applications will be running on the same machine as SQL Server, and therefore, it's probably safe to disable the short file name generator in NTFS by using the fsutil command line utility as shown here. This won't improve performance for reading existing files, but it can have a significant impact if you are writing many files because NTFS slows down more and more as it works harder to generate a unique short named version for each file in a folder with many files. In benchmark tests, I've seen performance take a hit with short file names enabled once you reach about 300,000 files in a single folder, and remember that FILESTREAM stores all the BLOB data from a single varbinary(max) column inside a single folder. Another NTFS feature that can affect performance is the last access time. Even when you just read a file, NTFS modifies the last access time to track every time the file is accessed. This is needless overhead in the case of FILESTREAM, and can negatively impact performance when dealing with a large number of files that are frequently read. So if the NTFS volume is being used only for FILESTREAM, which by the way is recommended practice, then you should use the fsutil command shown here to disable the last access time update. When you format an NTFS volume, you can set its cluster size. The cluster size represents the unit of allocation on disk, and by default, NTFS uses a cluster size of 4K. This means that, by default, a file that's under 4K in size, even if it's only 1 byte, will consume a full 4K on disk, and the moment that file grows just 1 byte over 4K, then another cluster gets allocated and now the file consumes 8K of disk space in two clusters. There is a trade-off at play when you set different cluster sizes. On the one hand, a smaller cluster size means less wasted disk space, especially when dealing with larger numbers of small files. But on the other hand, a larger cluster size means less I/O, which translates, of course, into better performance, especially when dealing with large BLOBs. Of course, you need to take your own usage patterns into account, but the general recommendation with FILESTREAM is for performance to win out over the concern for wasted disk space and to use 64K clusters, which is the largest cluster size supported by NTFS. For example, this format statement prepares a new NTFS volume on the F drive with a cluster size of 64K. Finally, and I'll mention this even though it may be obvious, a badly fragmented disk will always negatively affect performance. So just like any other disk volume, it's good practice to routinely defragment the NTFS volumes that are being used for your FILESTREAM containers.